{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fcc806b-db2d-4ca7-8d34-6763310fab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import gensim\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import TensorDataset \n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from encoder import DocuEncoder, ClassEncoder, DocumentTokenizer\n",
    "from layer import GCN\n",
    "from classifier import TextClassifier\n",
    "from preprocessor import TaxoDataManager, DocumentManager\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_CLASSIFIER_DIR = f'{os.path.dirname(os.path.abspath(__file__))}'\n",
    "DATA_ROOT = os.path.join(TEXT_CLASSIFIER_DIR, 'data')\n",
    "TRAINING_DATA_DIR = os.path.join(DATA_ROOT, 'training_data/amazon/')\n",
    "TOKEN_LENGTH = 500\n",
    "CLASS_LENGTH = 768\n",
    "\n",
    "word2vec_model = word2vec.Word2Vec.load(os.path.join(DATA_ROOT, 'pretrained/embedding'))\n",
    "\n",
    "taxo_manager = TaxoDataManager(TRAINING_DATA_DIR, 'taxonomy.json', 'amazon', word2vec_model)\n",
    "taxo_manager.load_all()\n",
    "\n",
    "document_tokenizer = DocumentTokenizer(DATA_ROOT, TOKEN_LENGTH)\n",
    "graph = taxo_manager.get_graph().to('cuda:0')\n",
    "features = taxo_manager.get_feature().cuda()\n",
    "\n",
    "dim = word2vec_model.wv.vector_size\n",
    "gcn = GCN(dim, dim, dim, 2, nn.ReLU)\n",
    "class_encoder = ClassEncoder(gcn, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba6bc0-9a59-46a0-9f82-05235e1708d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(data_name, document_file, token_length, batch_size):\n",
    "    training_data_dir = os.path.join(DATA_ROOT, f'training_data/{data_name}/')\n",
    "    training_document_manager = DocumentManager(document_file, training_data_dir, f'{data_name}_train', document_tokenizer.Tokenize, taxo_manager)\n",
    "    training_document_manager.load_tokens()\n",
    "    training_document_manager.load_dicts()\n",
    "\n",
    "    graph = taxo_manager.get_graph().to('cuda:0')\n",
    "    num_classes = len(graph.nodes())\n",
    "    training_document_ids = training_document_manager.get_ids()\n",
    "\n",
    "    for i, document_id in enumerate(training_document_ids, 0):\n",
    "        tokens = torch.tensor(training_document_manager.get_tokens(document_id), dtype=torch.int32)\n",
    "        tokens = torch.reshape(tokens, (-1, 1))\n",
    "        positive, non_negative = training_document_manager.get_output_label(document_id)\n",
    "        output = torch.zeros(num_classes, 1)\n",
    "        mask = torch.ones(num_classes, 1, dtype=torch.int32)\n",
    "\n",
    "        for j in non_negative:\n",
    "            if j in positive:\n",
    "                output[j][0] = 1\n",
    "            else:\n",
    "                mask[j] = 0\n",
    "            input = torch.cat((tokens, mask), 0)\n",
    "            if i==0:\n",
    "                train_x = input\n",
    "                train_y = output\n",
    "            else:\n",
    "                train_x = torch.cat((train_x, input), 0)\n",
    "                train_y = torch.cat((train_y, output),0)\n",
    "\n",
    "    train_x = torch.reshape(train_x, (-1, num_classes + token_length))\n",
    "    train_y = torch.reshape(train_y, (-1, num_classes, 1))\n",
    "\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "\n",
    "    return DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1310748-ce34-416e-b844-48ecde5c70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_coreclass(text_classifier, epoch, data_loader, loss_function, optimizer):\n",
    "    text_classifier.cuda()\n",
    "    text_classifier.train()\n",
    "\n",
    "    for epoch in range(epoch): \n",
    "        start = time.time()\n",
    "        running_loss = 0.0\n",
    "        batch_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        for i, train_data in enumerate(data_loader):\n",
    "            inputs, outputs = train_data\n",
    "            predicted = text_classifier(inputs.cuda())\n",
    "            loss = loss_function(predicted, outputs.cuda())\n",
    "            batch_loss = loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            if (i+1) % 8 == 0 :\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                print('[%d, %5d] batch loss: %.3f' % (epoch + 1, i + 1, batch_loss))\n",
    "            running_loss += batch_loss\n",
    "\n",
    "\n",
    "        print('[%d] total loss: %.3f' % (epoch + 1, running_loss))\n",
    "        print('elapsed time : %f'%(time.time()-start))\n",
    "\n",
    "    print('Finished Core Class Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4e67c-2bbe-48b5-8059-5e0b016667b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(prediction):\n",
    "    weight = prediction ** 2 / prediction.sum(axis=0)\n",
    "    weight_1 = (1 - prediction) **2 / (1 - prediction).sum(axis=0)\n",
    "    return weight / (weight + weight_1)\n",
    "\n",
    "def train_self(text_classifier, epoch, data_loader, loss_function, optimizer, update_period=25):\n",
    "    for _ in range(epoch): \n",
    "        start = time.time()\n",
    "        running_loss = 0.0\n",
    "        batch_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        target = None\n",
    "\n",
    "        for i, train_data in enumerate(data_loader):\n",
    "            inputs, outputs = train_data\n",
    "            predicted = text_classifier(inputs.cuda())\n",
    "            if i % update_period == 0:\n",
    "                target = target_distribution(predicted) \n",
    "            loss = loss_function(target, predicted)\n",
    "            loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e309f-4ef0-49ae-a931-c898304e45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with amazon\n",
    "data_loader = create_data_loader(\n",
    "  'amazon', 'train-with-core-class-1000.jsonl', token_length=500, batch_size=4)\n",
    "\n",
    "text_classifier = TextClassifier(class_encoder, DocuEncoder(DATA_ROOT), (dim, CLASS_LENGTH), TOKEN_LENGTH, graph, features, nn.Sigmoid(), False)\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "  {'params': text_classifier.document_encoder.parameters(), 'lr': 5e-5},\n",
    "  {'params': text_classifier.class_encoder.parameters()},\n",
    "  {'params': text_classifier.weight}], lr=4e-3)\n",
    "\n",
    "train_coreclass(text_classifier, 20, data_loader, torch.nn.BCELoss(reduction='sum'), optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:text-classifier] *",
   "language": "python",
   "name": "conda-env-text-classifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
