{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc806b-db2d-4ca7-8d34-6763310fab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import gensim\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch import nn\n",
    "from encoder import DocuEncoder, ClassEncoder, DocumentTokenizer\n",
    "from layer import GCN\n",
    "from classifier import TextClassifier\n",
    "from preprocessor import TaxoDataManager, DocumentManager\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b691d-58d7-41a1-8d1e-972a096f0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import discord_notify as dn\n",
    "\n",
    "WEBHOOK_URL = 'https://discord.com/api/webhooks/917284193036275712/2Da9DmvQjYugyP8pzvB4AzPMqVEizyVipHYLDPE79ZySU2aPGL3imH-YdcqkiUZxf_ku'\n",
    "\n",
    "notifier = dn.Notifier(WEBHOOK_URL)\n",
    "# notifier.send(\"노트북 실행 시작: train_amazon.ipynb\", print_message=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = 'data/'\n",
    "TRAINING_DATA_DIR = os.path.join(DATA_ROOT, 'training_data/amazon/')\n",
    "TOKEN_LENGTH = 500\n",
    "CLASS_LENGTH = 768\n",
    "\n",
    "word2vec_model = word2vec.Word2Vec.load(os.path.join(DATA_ROOT, 'pretrained/embedding'))\n",
    "\n",
    "taxo_manager = TaxoDataManager(TRAINING_DATA_DIR, 'taxonomy.json', 'amazon', word2vec_model)\n",
    "taxo_manager.load_all()\n",
    "\n",
    "document_tokenizer = DocumentTokenizer(DATA_ROOT, TOKEN_LENGTH)\n",
    "graph = taxo_manager.get_graph().to('cuda:0')\n",
    "features = taxo_manager.get_feature().cuda()\n",
    "\n",
    "dim = word2vec_model.wv.vector_size\n",
    "gcn = GCN(dim, dim, dim, 2, nn.ReLU())\n",
    "class_encoder = ClassEncoder(gcn, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba6bc0-9a59-46a0-9f82-05235e1708d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data_name, document_file, token_length, num_val=None):\n",
    "    elapsed_start = time.time()\n",
    "    training_data_dir = os.path.join(DATA_ROOT, f'training_data/{data_name}/')\n",
    "    training_document_manager = DocumentManager(document_file, training_data_dir, f'{data_name}_train', document_tokenizer.Tokenize, taxo_manager, force_token_reload=True)\n",
    "    training_document_manager.load_tokens()\n",
    "    training_document_manager.load_dicts()\n",
    "\n",
    "    num_classes = len(graph.nodes())\n",
    "    training_document_ids = training_document_manager.get_ids()\n",
    "\n",
    "    for i, document_id in enumerate(training_document_ids, 0):\n",
    "        tokens = torch.tensor(training_document_manager.get_tokens(document_id), dtype=torch.int32)\n",
    "        tokens = torch.reshape(tokens, (-1, 1))\n",
    "        positive, non_negative = training_document_manager.get_output_label(document_id)\n",
    "        output = torch.zeros(num_classes, 1)\n",
    "        mask = torch.ones(num_classes, 1, dtype=torch.int32)\n",
    "\n",
    "        for j in non_negative:\n",
    "            if j in positive:\n",
    "                output[j][0] = 1\n",
    "            else:\n",
    "                mask[j] = 0\n",
    "        input = torch.cat((tokens, mask), 0)\n",
    "        if i==0:\n",
    "            train_x = input\n",
    "            train_y = output\n",
    "        else:\n",
    "            train_x = torch.cat((train_x, input), 0)\n",
    "            train_y = torch.cat((train_y, output), 0)\n",
    "        \n",
    "    train_x = torch.reshape(train_x, (-1, num_classes + token_length))\n",
    "    train_y = torch.reshape(train_y, (-1, num_classes, 1))\n",
    "\n",
    "    dataset = TensorDataset(train_x, train_y)\n",
    "    num_dataset = len(dataset)\n",
    "    if num_val is not None:\n",
    "        dataset = random_split(dataset, [num_val, num_dataset - num_val])\n",
    "    \n",
    "    notifier.send(f'{num_dataset}개 데이터셋 생성 완료. 걸린 시간: {round(time.time() - elapsed_start, 2)}.')\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "def validate_coreclass(model, dataloader, criterion):\n",
    "    valid_loss = 0.0\n",
    "    for data, labels in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            data, labels = data.cuda(), labels.cuda()\n",
    "        \n",
    "        target = model(data)\n",
    "        loss = criterion(target,labels)\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "    return valid_loss / len(dataloader)\n",
    "\n",
    "def train_coreclass(text_classifier, epoch, data_loader, loss_function, optimizer, valid_dataloader = None, save_path = None):\n",
    "    text_classifier.cuda()\n",
    "    train_start = time.time()\n",
    "    min_valid_loss = np.inf\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        start = time.time()\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        for i, train_data in enumerate(data_loader):\n",
    "            inputs, outputs = train_data\n",
    "            predicted = text_classifier(inputs.cuda())\n",
    "            loss = loss_function(predicted, outputs.cuda())\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i+1) % 8 == 0 :\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                # print('[%d, %5d] batch loss: %.3f' % (e + 1, i + 1, batch_loss))\n",
    "                \n",
    "        running_loss /= len(data_loader)\n",
    "\n",
    "        valid_loss = validate_coreclass(text_classifier, valid_dataloader, loss_function)\n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) Saving The Model')\n",
    "            min_valid_loss = valid_loss\n",
    "            torch.save(text_classifier.state_dict(), save_path)\n",
    "\n",
    "        print(f'[{e + 1}] train loss: {round(running_loss, 3)}. validation_loss: {round(valid_loss, 3)}. elapsed time: {time.time() - start}')\n",
    "\n",
    "\n",
    "    notifier.send(f'{epoch} epoch 코어클래스 학습 완료. 걸린 시간: {round(time.time() - train_start, 2)}.')\n",
    "    \n",
    "def safe_div(a, b, epsilon=1e-8):\n",
    "    return a / b.clamp(min=epsilon)\n",
    "\n",
    "def safe_log(a, epsilon=1e-8):\n",
    "    return torch.log(a.clamp(min=epsilon))\n",
    "\n",
    "def target_distribution(prediction):\n",
    "    weight = safe_div(prediction ** 2, prediction.sum(axis=0))\n",
    "    weight_1 = safe_div((1 - prediction) **2, (1 - prediction).sum(axis=0))\n",
    "    return safe_div(weight, (weight + weight_1))\n",
    "\n",
    "def validate_self(model, dataloader, criterion):\n",
    "    valid_loss = 0.0\n",
    "    for data, _ in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "        \n",
    "        predicted = model(data)\n",
    "        target = target_distribution(predicted) \n",
    "        loss = criterion(predicted, target)\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "    return valid_loss / len(dataloader)\n",
    "\n",
    "def train_self(text_classifier, epoch, data_loader, loss_function, optimizer, update_period, valid_dataloader, save_path):\n",
    "    text_classifier.cuda()\n",
    "    train_start = time.time()\n",
    "    min_valid_loss = np.inf\n",
    "    \n",
    "    for e in range(epoch): \n",
    "        start = time.time()\n",
    "        running_loss = 0.0\n",
    "        batch_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, train_data in enumerate(data_loader):\n",
    "            inputs, outputs = train_data\n",
    "            predicted = text_classifier(inputs.cuda())\n",
    "            target = target_distribution(predicted)\n",
    "            loss = loss_function(predicted, target)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            if i % update_period == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        running_loss /= len(data_loader)\n",
    "        valid_loss = validate_self(text_classifier, valid_dataloader, loss_function)\n",
    "        if min_valid_loss > valid_loss:\n",
    "            print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) Saving The Model')\n",
    "            min_valid_loss = valid_loss\n",
    "            torch.save(text_classifier.state_dict(), save_path)\n",
    "\n",
    "        print(f'[{e + 1}] train loss: {round(running_loss, 3)}. validation_loss: {round(valid_loss, 3)}. elapsed time: {time.time() - start}')\n",
    "        \n",
    "    notifier.send(f'{epoch} epoch 자기 학습 완료. 걸린 시간: {round(time.time() - train_start, 2)}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e309f-4ef0-49ae-a931-c898304e45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with amazon\n",
    "val_dataset, train_dataset = create_dataset('amazon', 'amazon-coreclass-45000.jsonl', token_length=500, num_val=5000)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49eb543-b260-4a5e-b7db-69ef176984fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = TextClassifier(class_encoder, DocuEncoder(DATA_ROOT), (dim, CLASS_LENGTH), TOKEN_LENGTH, graph, features, nn.Sigmoid(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67996cd-b61f-4792-ba20-8f2ca78ad04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW([\n",
    "  {'params': text_classifier.document_encoder.parameters(), 'lr': 5e-5},\n",
    "  {'params': text_classifier.class_encoder.parameters()},\n",
    "  {'params': text_classifier.weight}], lr=4e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b4b24-783e-40cc-ad82-ea3d53c723ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(DATA_ROOT, 'trained/text-classifier-amazon.pt')\n",
    "train_coreclass(text_classifier, 20, train_dataloader, torch.nn.BCELoss(reduction='sum'), optimizer, val_dataloader, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60384a-3868-4ea4-a91b-b85fbb4edb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_loss(predicted, target):\n",
    "    return (target * safe_log(safe_div(target, predicted))).sum()\n",
    "\n",
    "train_self(text_classifier, 5, train_dataloader, kl_div_loss, optimizer, 25, val_dataloader, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122aa0b-c7ef-44ff-b364-57065c3e30ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:text-classifier] *",
   "language": "python",
   "name": "conda-env-text-classifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
