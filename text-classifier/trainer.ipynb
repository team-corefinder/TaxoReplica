{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fcc806b-db2d-4ca7-8d34-6763310fab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import gensim\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "from torch.utils.data import TensorDataset \n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from encoder import DocuEncoder, ClassEncoder, DocumentTokenizer\n",
    "from layer import GCN\n",
    "from classifier import TextClassifier\n",
    "from preprocessor import TaxoDataManager, DocumentManager\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba6bc0-9a59-46a0-9f82-05235e1708d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_CLASSIFIER_DIR = f'{os.path.dirname(os.path.abspath(__file__))}'\n",
    "DATA_ROOT = os.path.join(TEXT_CLASSIFIER_DIR, 'data')\n",
    "\n",
    "def create_data_loader(data_name, taxo_file, document_file, token_length, batch_size):\n",
    "    document_tokenizer = DocumentTokenizer(DATA_ROOT, token_length)\n",
    "\n",
    "    word2vec_model = word2vec.Word2Vec.load(os.path.join(DATA_ROOT, 'pretrained/embedding'))\n",
    "    taxo_manager = TaxoDataManager(dir, taxo_file, data_name, word2vec_model)\n",
    "    taxo_manager.load_all()\n",
    "\n",
    "\n",
    "    training_data_dir = os.path.join(DATA_ROOT, f'training_data/{data_name}/')\n",
    "    training_document_manager = DocumentManager(document_file, training_data_dir, f'{data_name}_train', document_tokenizer.Tokenize, taxo_manager)\n",
    "    training_document_manager.load_tokens()\n",
    "    training_document_manager.load_dicts()\n",
    "\n",
    "    graph = taxo_manager.get_graph().to('cuda:0')\n",
    "    num_classes = len(graph.nodes())\n",
    "    training_document_ids = training_document_manager.get_ids()\n",
    "\n",
    "    for i, document_id in enumerate(training_document_ids, 0):\n",
    "        tokens = torch.tensor(training_document_manager.get_tokens(document_id), dtype=torch.int32)\n",
    "        tokens = torch.reshape(tokens, (-1, 1))\n",
    "        positive, non_negative = training_document_manager.get_output_label(document_id)\n",
    "        output = torch.zeros(num_classes, 1)\n",
    "        mask = torch.ones(num_classes, 1, dtype=torch.int32)\n",
    "\n",
    "        for j in non_negative:\n",
    "            if j in positive:\n",
    "                output[j][0] = 1\n",
    "            else:\n",
    "                mask[j] = 0\n",
    "            input = torch.cat((tokens, mask), 0)\n",
    "            if i==0:\n",
    "                train_x = input\n",
    "                train_y = output\n",
    "            else:\n",
    "                train_x = torch.cat((train_x, input), 0)\n",
    "                train_y = torch.cat((train_y, output),0)\n",
    "\n",
    "    train_x = torch.reshape(train_x, (-1, num_classes + token_length))\n",
    "    train_y = torch.reshape(train_y, (-1, num_classes, 1))\n",
    "\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "\n",
    "    return DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1310748-ce34-416e-b844-48ecde5c70e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coreclass_train(text_classifier, epoch, data_loader, loss_function, optimizer):\n",
    "    text_classifier.cuda()\n",
    "    text_classifier.train()\n",
    "\n",
    "    for epoch in range(epoch): \n",
    "        start = time.time()\n",
    "        running_loss = 0.0\n",
    "        batch_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        for i, train_data in enumerate(data_loader):\n",
    "            inputs, outputs = train_data\n",
    "            predicted = text_classifier(inputs.cuda())\n",
    "            loss = loss_function(predicted, outputs.cuda())\n",
    "            batch_loss = loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            if (i+1) % 8 == 0 :\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                print('[%d, %5d] batch loss: %.3f' % (epoch + 1, i + 1, batch_loss))\n",
    "            running_loss += batch_loss\n",
    "\n",
    "\n",
    "        print('[%d] total loss: %.3f' % (epoch + 1, running_loss))\n",
    "        print('elapsed time : %f'%(time.time()-start))\n",
    "\n",
    "    print('Finished Core Class Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4e67c-2bbe-48b5-8059-5e0b016667b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(prediction):\n",
    "    weight = prediction ** 2 / prediction.sum(axis=0)\n",
    "    weight_1 = (1 - prediction) **2 / (1 - prediction).sum(axis=0)\n",
    "    return weight / (weight + weight_1)\n",
    "\n",
    "def self_train(text_classifier, epoch, data_loader, loss_function, optimizer, update_period=25):\n",
    "    for _ in range(epoch): \n",
    "        start = time.time()\n",
    "        running_loss = 0.0\n",
    "        batch_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        target = None\n",
    "\n",
    "        for i, train_data in enumerate(data_loader):\n",
    "            inputs, outputs = train_data\n",
    "            predicted = text_classifier(inputs.cuda())\n",
    "            if i % update_period == 0:\n",
    "                target = target_distribution(predicted) \n",
    "            loss = loss_function(target, predicted)\n",
    "            loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e309f-4ef0-49ae-a931-c898304e45f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with amazon        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd488fc-a0b2-4583-ab9d-0abfac733848",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "  def __init__(self, dir, train_file, taxonomy_file, data_name,\n",
    "    bert_lr, others_lr, token_length, cls_length, batch_size, epoch, activation = nn.Sigmoid(), rescaling = False, test_file = None):\n",
    "\n",
    "    self.dir = dir\n",
    "    self.train_file = train_file\n",
    "    if test_file != None:\n",
    "      self.test_file = test_file\n",
    "    else:\n",
    "      self.test_file = 0\n",
    "    self.taxonomy_file = taxonomy_file\n",
    "    self.data_name = data_name\n",
    "    #Document encoding input max length = token length, output length = cls_length, which is cls token's length.\n",
    "    self.T = token_length\n",
    "    self.C = cls_length\n",
    "\n",
    "    #hyper parameter for training\n",
    "    self.bert_lr = bert_lr\n",
    "    self.others_lr = others_lr\n",
    "    self.B = batch_size\n",
    "    self.epoch = epoch\n",
    "    self.activation = activation\n",
    "    self.rescaling = rescaling\n",
    "            \n",
    "\n",
    "  def prepare_train(self):\n",
    "\n",
    "    #document encoder\n",
    "    self.d_encoder = DocuEncoder(dir)\n",
    "    self.d_tokenizer = DocumentTokenizer(dir, self.T)\n",
    "\n",
    "    #word embedding model for class encoder\n",
    "    word2vec_model = word2vec.Word2Vec.load(self.dir + 'pretrained/' + 'embedding')\n",
    "    self.W = word2vec_model.wv.vector_size\n",
    "\n",
    "    #create gcn for class encoder. input_dim = W, hidden_dim = W, output_dim = W\n",
    "    gcn_model = GCN(self.W, self.W, self.W, 2, nn.ReLU())\n",
    "\n",
    "    self.class_encoder = ClassEncoder(gcn_model, word2vec_model)\n",
    "\n",
    "    #TaxoDataManager load and manage taxonomy information of dataset.\n",
    "    self.tm = TaxoDataManager( self.dir + 'training_data/'+self.data_name + '/', self.taxonomy_file, self.data_name, word2vec_model)\n",
    "    self.tm.load_all()\n",
    "\n",
    "    self.train_dm = DocumentManager(self.train_file, self.dir + 'training_data/' + self.data_name + '/' ,\n",
    "      self.data_name + '_train', self.d_tokenizer.Tokenize,  self.tm)\n",
    "\n",
    "    if self.test_file:\n",
    "      self.test_dm = DocumentManager(self.test_file, self.dir + 'training_data/' + self.data_name + '/' ,\n",
    "        self.data_name + '_test', self.d_tokenizer.Tokenize,  self.tm)\n",
    "    else:\n",
    "      self.test_dm = 0\n",
    "    \n",
    "    self.train_dm.load_tokens()\n",
    "    self.train_dm.load_dicts()\n",
    "\n",
    "    #g is graph of the taxonomy structure.\n",
    "    self.g = self.tm.get_graph().to('cuda:0')\n",
    "    #L is the number of the classes.\n",
    "    self.L = len(self.g.nodes())\n",
    "\n",
    "    #feature is L x W matrix, word embedding of the classes.\n",
    "    self.features = self.tm.get_feature().cuda()\n",
    "\n",
    "    self.text_classifier = TextClassifier(self.class_encoder, self.d_encoder, \n",
    "      (self.W, self.C), self.T, self.g,\n",
    "      self.features, self.activation, self.rescaling)\n",
    "\n",
    "    sum = 0\n",
    "    for c in gcn_model.parameters():\n",
    "      sum = sum + 1\n",
    "    print(\"GCN total Layer: \", sum)\n",
    "\n",
    "\n",
    "\n",
    "    sum = 0\n",
    "    for c in self.text_classifier.parameters():\n",
    "      sum = sum + 1\n",
    "    print(\"Text-classfier Layer: \", sum)\n",
    "\n",
    "\n",
    "    #set loss function and optimizer for training\n",
    "    self.loss_fun = torch.nn.BCELoss(reduction='sum')\n",
    "    self.loss_kl = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "    self.optimizer_kl = optim.AdamW([\n",
    "      {'params': self.text_classifier.document_encoder.parameters(), 'lr': self.bert_lr},\n",
    "      {'params': self.text_classifier.class_encoder.parameters()},\n",
    "      {'params': self.text_classifier.weight}], lr=self.others_lr)\n",
    "    self.optimizer = optim.AdamW([\n",
    "      {'params': self.text_classifier.document_encoder.parameters(), 'lr': self.bert_lr},\n",
    "      {'params': self.text_classifier.class_encoder.parameters()},\n",
    "      {'params': self.text_classifier.weight}], lr=self.others_lr)\n",
    "    \n",
    "    train_ids = self.train_dm.get_ids()\n",
    "\n",
    "    #generate train set\n",
    "    for i, document_id in enumerate(train_ids, 0):\n",
    "      tokens = torch.tensor( self.train_dm.get_tokens(document_id) ,dtype = torch.int32)\n",
    "      tokens = torch.reshape(tokens, (-1, 1))\n",
    "      pos, nonneg = self.train_dm.get_output_label(document_id)\n",
    "      output = torch.zeros(self.L,1)\n",
    "      mask = torch.ones(self.L,1, dtype = torch.int32)\n",
    "\n",
    "      for j in nonneg:\n",
    "        if j in pos:\n",
    "          output[j][0] = 1\n",
    "        else :\n",
    "          mask[j] = 0\n",
    "      input = torch.cat((tokens, mask), 0)\n",
    "      if i==0:\n",
    "        train_x = input\n",
    "        train_y = output\n",
    "      else:\n",
    "        train_x = torch.cat((train_x, input), 0)\n",
    "        train_y = torch.cat((train_y, output),0)\n",
    "    \n",
    "    train_x = torch.reshape(train_x, ( -1, self.L + self.T ))\n",
    "    train_y = torch.reshape(train_y, ( -1, self.L, 1 ))\n",
    "\n",
    "\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "\n",
    "\n",
    "    self.train_dataloader = DataLoader(train_dataset, batch_size=self.B, shuffle=True)\n",
    "            \n",
    "    \n",
    "  def train(self):\n",
    "    print(\"Start training! bert learning rate: %f, other learning rate: %f, epoch: %d, batch size: %d\"\n",
    "      %(self.bert_lr, self.others_lr, self.epoch, self.B))\n",
    "    self.text_classifier.cuda()\n",
    "    self.text_classifier.train()\n",
    "\n",
    "    for epoch in range(self.epoch): \n",
    "      start = time.time()\n",
    "      running_loss = 0.0\n",
    "      batch_loss = 0.0\n",
    "      self.optimizer.zero_grad()\n",
    "      self.optimizer_kl.zero_grad()\n",
    "      for i, train_data in enumerate(self.train_dataloader):\n",
    "        inputs, outputs = train_data\n",
    "        predicted = self.text_classifier(inputs.cuda())\n",
    "        loss = self.loss_fun(predicted, outputs.cuda())\n",
    "        batch_loss += loss.item()\n",
    "        if (i+1) % 25 == 0:\n",
    "          print('Start self-training...')\n",
    "          weight = predicted**2 / predicted.sum(axis=0)\n",
    "          #q = (weight.T / weight.sum(axis=1).T)\n",
    "          weight_1 = (1-predicted)**2 / (1-predicted).sum(axis=0)\n",
    "          q = weight / (weight + weight_1)\n",
    "          # shape of predicted = (Batch size, the number of labels, 1)\n",
    "          loss_kl = self.loss_kl(q, predicted)\n",
    "          (loss+loss_kl).backward()\n",
    "          self.optimizer_kl.step()\n",
    "          self.optimizer_kl.zero_grad()\n",
    "          print('[%d, %5d] KL loss: %.3f' %\n",
    "                (epoch + 1, i + 1, loss_kl.item()))\n",
    "                \n",
    "        else:\n",
    "          loss.backward()\n",
    "\n",
    "\n",
    "        if (i+1)%8 == 0 :\n",
    "          #print(\"optimized!\")\n",
    "          self.optimizer.step()\n",
    "          self.optimizer.zero_grad()\n",
    "          print('[%d, %5d] batch loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, batch_loss ))\n",
    "        running_loss += batch_loss\n",
    "        batch_loss = 0.0\n",
    "\n",
    "              \n",
    "\n",
    "\n",
    "      print('[%d] total loss: %.3f' %\n",
    "              (epoch + 1, running_loss ))\n",
    "      print('elapsed time : %f'%(time.time()-start))\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a7e13-627f-419e-8f3b-e97a0641b8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "        \n",
    "  #dir = './'\n",
    "\n",
    "  root = os.path.dirname(os.path.abspath(__file__)) \n",
    "\n",
    "  if not os.path.isdir(root + \"/data\"):\n",
    "    url = \"https://drive.google.com/drive/folders/1K6oXC2lKZdNcFaPVuCnozHojhBLSfcfb\"\n",
    "    output =\"data\"\n",
    "    gdown.download_folder(url, output= output)\n",
    "\n",
    "  dir = root + \"/data/\"\n",
    "\n",
    "  \"\"\"\n",
    "  #DBPedia dataset\n",
    "  train_file = 'DBPEDIA_30000_coreclass.jsonl'\n",
    "  taxonomy_file = 'taxonomy.json'\n",
    "  data_name = 'DBPEDIA'\n",
    "  \"\"\"\n",
    "\n",
    "  \n",
    "  #amazon dataset\n",
    "  train_file = 'train-with-core-class-1000.jsonl'\n",
    "  taxonomy_file = 'taxonomy.json'\n",
    "  data_name = 'amazon'\n",
    "  \n",
    "  bert_lr = 5e-5\n",
    "  others_lr = 4e-3\n",
    "  token_length = 500\n",
    "  batch_size =4\n",
    "  epoch = 20\n",
    "  cls_length = 768\n",
    "  \n",
    "  #default activation function is Sigmoid\n",
    "  activation = nn.Sigmoid()\n",
    "  #activation = nn.Softmax(dim = 1)\n",
    "\n",
    "  rescaling = False\n",
    "\n",
    "  trainer = Trainer(dir, train_file, taxonomy_file, data_name,\n",
    "    bert_lr, others_lr, token_length, cls_length, \n",
    "    batch_size, epoch, activation, rescaling)\n",
    "\n",
    "  trainer.prepare_train()\n",
    "  trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:text-classifier] *",
   "language": "python",
   "name": "conda-env-text-classifier-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
