{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from urllib.request import urlopen\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from statistics import median\n",
        "import time\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5byqsslaRDXU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda not supported in your platform\n"
          ]
        }
      ],
      "source": [
        "ROBERTA_DIR = '../roberta.large.mnli'\n",
        "assert os.path.exists(f'{ROBERTA_DIR}/model.pt')\n",
        "roberta = RobertaModel.from_pretrained(ROBERTA_DIR, checkpoint_file='model.pt')\n",
        "try:\n",
        "  roberta.cuda()\n",
        "except:\n",
        "  print('cuda not supported in your platform')\n",
        "roberta.eval()  # disable dropout (or leave in train mode to finetune)\n",
        "\n",
        "def similarity(text, class_name):\n",
        "  tokens = roberta.encode(text, f'this document is about {class_name.lower()}')\n",
        "  logits = roberta.predict('mnli', tokens[:512], return_logits=True)\n",
        "  probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "  entailment_probability = probabilities[2]\n",
        "  return entailment_probability\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nFXvWCYzleqK"
      },
      "outputs": [],
      "source": [
        "caches = dict()\n",
        "\n",
        "class Node:\n",
        "  def __init__(self, name, dic, parent, depth):\n",
        "    self.name = name\n",
        "    self.dic = dic\n",
        "    self.parent = parent\n",
        "    self.depth = depth\n",
        "    self.cache = dict()\n",
        "\n",
        "  def full_path(self):\n",
        "    path = [self.name]\n",
        "    cursor = self.parent\n",
        "    while cursor is not None:\n",
        "      path.append(cursor.name)\n",
        "      cursor = cursor.parent\n",
        "    return reversed(path)\n",
        "\n",
        "  def children(self):\n",
        "    return [Node(k, v, self, self.depth + 1) for k, v in self.dic.items()]\n",
        "  \n",
        "  def selected_children(self, doc):\n",
        "    return sorted(self.children(), key=lambda c: c.similarity(doc), reverse=True)[:(self.depth + 2)]\n",
        "\n",
        "  def similarity(self, text):\n",
        "    cache = caches.get(text, dict())\n",
        "    if text not in caches:\n",
        "      caches[text] = cache\n",
        "    if self.name not in cache:\n",
        "      cache[self.name] = similarity(text, self.name)\n",
        "    return cache[self.name]\n",
        "  \n",
        "  def path_score(self, doc):\n",
        "    if self.parent is None:\n",
        "      return 1\n",
        "    \n",
        "    return self.parent.path_score(doc) * self.similarity(doc)\n",
        "\n",
        "  def confidence(self, text):\n",
        "    competitors = [self.parent] + self.parent.children()\n",
        "    return self.similarity(text) - max([n.similarity(text) for n in competitors])\n",
        "\n",
        "  def confidence_threshold(self, all_documents):\n",
        "    return median([self.confidence(doc.text) for doc in all_documents if doc.tagged_with(self.name)])\n",
        "\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "  return [item for l in list_of_lists for item in l]\n",
        "\n",
        "def aggregate_children(children_list, doc):\n",
        "  children = flatten(children_list)\n",
        "  if not children:\n",
        "    return children\n",
        "  depth = children[0].depth\n",
        "  return sorted(children, key=lambda n: n.path_score(doc), reverse=True)[:((depth + 1) ** 2)]\n",
        "\n",
        "def deeper_nodes(nodes, doc):\n",
        "  children_list = [n.selected_children(doc) for n in nodes]\n",
        "  return aggregate_children(children_list, doc)\n",
        "\n",
        "def get_candidates(doc, tree):\n",
        "  root = Node('root', tree, None, 0)\n",
        "\n",
        "  depth1 = root.selected_children(doc)\n",
        "\n",
        "  candidates = []\n",
        "  nodes = depth1\n",
        "\n",
        "  while nodes:\n",
        "    candidates = candidates + nodes\n",
        "    nodes = deeper_nodes(nodes, doc)\n",
        "\n",
        "  return candidates\n",
        "\n",
        "class Doc:\n",
        "  def __init__(self, review, tree):\n",
        "    text = review['reviewText']\n",
        "    self.text = text\n",
        "    self.review = review\n",
        "    self.candidates = get_candidates(text, tree)\n",
        "    self.class_names = {n.name for n in self.candidates}\n",
        "\n",
        "  def tagged_with(self, name):\n",
        "    return name in self.class_names\n",
        "\n",
        "  def core_classes(self, all_documents):\n",
        "    return [n.full_path() for n in self.candidates if n.name and n.confidence(self.text) >= n.confidence_threshold(all_documents)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oBtYOAWod2sZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 out of 10 complete taking 31.082374811172485 seconds\n"
          ]
        }
      ],
      "source": [
        "AMAZON_DATA_DIR = '../data/amazon'\n",
        "\n",
        "def get_reviews(filename = 'train-1000.jsonl'):\n",
        "  with open(f'{AMAZON_DATA_DIR}/{filename}') as reviewsFile:\n",
        "    for line in reviewsFile:\n",
        "      yield line.strip()\n",
        "\n",
        "def get_documents(reviews):\n",
        "  with open(f'{AMAZON_DATA_DIR}/taxonomy.json') as f:\n",
        "    tree = json.load(f)\n",
        "\n",
        "    num_workers = 8\n",
        "    batch_size = int(len(reviews) / num_workers)\n",
        "    data_loader = DataLoader(dataset=reviews, num_workers=num_workers, batch_size=batch_size)\n",
        "\n",
        "    for review_batch in data_loader:\n",
        "      for index, r in enumerate(review_batch):\n",
        "        start = time.time()\n",
        "        yield Doc(json.loads(r), tree)\n",
        "        end = time.time()\n",
        "        print(f'{index + 1} out of {len(reviews)} complete taking {end - start} seconds')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  reviews = list(get_reviews())[:10]\n",
        "  all_documents = list(get_documents(reviews))\n",
        "  with open(f'{AMAZON_DATA_DIR}/train-with-core-class-1000.jsonl', 'w') as f:\n",
        "    for doc in all_documents:\n",
        "      doc.review['core_classes'] = doc.core_classes(all_documents)\n",
        "      f.write(json.dumps(doc.review) + '\\n')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "core-class",
      "provenance": []
    },
    "interpreter": {
      "hash": "5cf75ca9a98a0fe76c822c9f024f38126059bdb47d45268cee682c16be0963eb"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('TaxoReplica': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
