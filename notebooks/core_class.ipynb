{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5byqsslaRDXU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from urllib.request import urlopen\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from statistics import median\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nFXvWCYzleqK"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = 'drive/MyDrive/taxo-replica'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oBtYOAWod2sZ"
      },
      "outputs": [],
      "source": [
        "dir = f'{BASE_DIR}/roberta.large.mnli'\n",
        "assert os.path.exists(f'{dir}/model.pt')\n",
        "roberta = RobertaModel.from_pretrained(dir, checkpoint_file='model.pt')\n",
        "roberta.eval()  # disable dropout (or leave in train mode to finetune)\n",
        "roberta.cuda()\n",
        "\n",
        "def similarity(text, class_name):\n",
        "  tokens = roberta.encode(text, f'this document is about {class_name.lower()}')\n",
        "  logits = roberta.predict('mnli', tokens, return_logits=True)\n",
        "  probabilities = logits.softmax(dim=-1).tolist()[0]\n",
        "  entailment_probability = probabilities[2]\n",
        "  return entailment_probability\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8CLc1uZaEP7l"
      },
      "outputs": [],
      "source": [
        "caches = dict()\n",
        "\n",
        "class Node:\n",
        "  def __init__(self, name, dic, parent, depth):\n",
        "    self.name = name\n",
        "    self.dic = dic\n",
        "    self.parent = parent\n",
        "    self.depth = depth\n",
        "    self.cache = dict()\n",
        "\n",
        "  def children(self):\n",
        "    return [Node(k, v, self, self.depth + 1) for k, v in self.dic.items()]\n",
        "  \n",
        "  def selected_children(self, doc):\n",
        "    return sorted(self.children(), key=lambda c: c.similarity(doc), reverse=True)[:(self.depth + 2)]\n",
        "\n",
        "  def similarity(self, text):\n",
        "    cache = caches.get(text, dict())\n",
        "    if text not in caches:\n",
        "      caches[text] = cache\n",
        "    if self.name not in cache:\n",
        "      cache[self.name] = similarity(text, self.name)\n",
        "    return cache[self.name]\n",
        "  \n",
        "  def path_score(self, doc):\n",
        "    if self.parent is None:\n",
        "      return 1\n",
        "    \n",
        "    return self.parent.path_score(doc) * self.similarity(doc)\n",
        "\n",
        "  def confidence(self, text):\n",
        "    competitors = [self.parent] + self.parent.children()\n",
        "    return self.similarity(text) - max([n.similarity(text) for n in competitors])\n",
        "\n",
        "  def confidence_threshold(self, all_documents):\n",
        "    return median([self.confidence(doc.text) for doc in all_documents if doc.tagged_with(self.name)])\n",
        "\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "  return [item for l in list_of_lists for item in l]\n",
        "\n",
        "def aggregate_children(children_list, doc):\n",
        "  children = flatten(children_list)\n",
        "  if not children:\n",
        "    return children\n",
        "  depth = children[0].depth\n",
        "  return sorted(children, key=lambda n: n.path_score(doc), reverse=True)[:((depth + 1) ** 2)]\n",
        "\n",
        "def deeper_nodes(nodes, doc):\n",
        "  children_list = [n.selected_children(doc) for n in nodes]\n",
        "  return aggregate_children(children_list, doc)\n",
        "\n",
        "def get_candidates(doc, tree):\n",
        "  root = Node('root', tree, None, 0)\n",
        "\n",
        "  depth1 = root.selected_children(doc)\n",
        "\n",
        "  candidates = []\n",
        "  nodes = depth1\n",
        "\n",
        "  while nodes:\n",
        "    candidates = candidates + nodes\n",
        "    nodes = deeper_nodes(nodes, doc)\n",
        "\n",
        "  return candidates\n",
        "\n",
        "class Doc:\n",
        "  def __init__(self, text, tree):\n",
        "    self.text = text\n",
        "    self.candidates = get_candidates(text, tree)\n",
        "    self.class_names = {n.name for n in self.candidates}\n",
        "\n",
        "  def tagged_with(self, name):\n",
        "    return name in self.class_names\n",
        "\n",
        "  def core_classes(self, all_documents):\n",
        "    return [n.name for n in self.candidates if n.confidence(self.text) >= n.confidence_threshold(all_documents)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKsrcILvnQAh"
      },
      "outputs": [],
      "source": [
        "def get_documents(corpus, tree):\n",
        "  all_documents = []\n",
        "\n",
        "  for text in corpus:\n",
        "    start = time.time()\n",
        "    all_documents.append(Doc(text, tree))\n",
        "    end = time.time()\n",
        "    print(f'{len(all_documents)} out of {len(corpus)} complete taking {end - start} seconds')\n",
        "\n",
        "  return all_documents\n",
        "\n",
        "def get_corpus():\n",
        "  with open(f'{BASE_DIR}/amazon/test.json') as reviewsFile:\n",
        "    reviews = json.load(reviewsFile)\n",
        "    corpus = [r['reviewText'] for r in reviews]\n",
        "    return corpus\n",
        "\n",
        "with open(f'{BASE_DIR}/amazon/taxonomy.json') as f:\n",
        "  corpus = get_corpus()[:10]\n",
        "  tree = json.load(f)\n",
        "  all_documents = get_documents(corpus, tree)\n",
        "\n",
        "  doc = all_documents[0]\n",
        "\n",
        "  print(doc.core_classes(all_documents))\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "core-class",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
